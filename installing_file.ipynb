{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste de commandes de création de notre env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancement de l'extraction des features textuelles : \n",
    "'''python feature_extraction/extract_txt_feats.py --btype robertabase --mvsa single --ht True'''\n",
    "# conda version : 23.1.0\n",
    "conda config --add channels conda-forge\n",
    "conda config --env --add channels pytorch\n",
    "conda install python=3.6.10  \n",
    "conda install pip\n",
    "pip install transformers==4.3.2\n",
    "conda install pytorch=1.7.1\n",
    "conda install torchvision=0.8.2\n",
    "pip install pandas==1.0.5\n",
    "pip install ekphrasis==0.5.1\n",
    "# regrouper images et textes dans 2 répertoires 'images' et 'texts' pour avoir 'data/mvsa_single/images/1.jpg', pour mvsa_single et mvsa_multiple, bien + d'images e textes pour multiple\n",
    "''' Et après ca ca a fonctionné '''\n",
    "\n",
    "# Lancement de l'excraction des features visuelles : \n",
    "'''python feature_extraction/extract_img_feats.py --vtype imagenet --mvsa single --ht False''' # ne pas laisser clip comme dans le readme, mais mettre imagenet a la place\n",
    "''' Devrait retourner un enchainement de 35 indices , prend un quart d'h'''\n",
    "\n",
    "# Lancement du training du modèle\n",
    "'''python train/multi_mlp_2mod.py --vtype clip --ttype clip --mvsa single --ht True --bs 32 --split 1'''\n",
    "# Devrait commencer par avoir un pb de nom de fichier 'train/multi_mlp_2mod.py', etait 'train/muti_mlp_2mod.py' (sans l) initialement\n",
    "pip install sklearn==0.0\n",
    "\n",
    "#Utilisation du modèle CLIP, combinaison aec ROBERTA pour le texte la plus precise\n",
    "# ne pas installer clip avec simplement pip install clip, cela ne donnera pas la bonne version de CLIP, notamment le module 'load' manquera\n",
    "# Installer comme il est indiqué dans le README du repo github du modèle CLIP\n",
    "conda install cudatoolkit #pour mac c'est suffisant, pour autre machine mettre cudatoolkit=11.0, et il faut bien pytorch 1.7.1\n",
    "pip install ftfy regex tqdm\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "# extraire les visuals features, devrait prendre environ 1H, compter 35 steps\n",
    "'''python feature_extraction/extract_img_feats.py --vtype clip --mvsa single --ht True'''\n",
    "\n",
    "#Extraction des features 'places' puis 'emotion'\n",
    "'''python feature_extraction/extract_img_feats.py --vtype places --mvsa single --ht True'''\n",
    "# Il faut avoir le fichier sous forme .tar directement comme on l'a téléchargé d'internet. 35 steps, environ 1h\n",
    "'''python feature_extraction/extract_img_feats.py --vtype emotion --mvsa single --ht True'''\n",
    "# Message d'erreur initiale recu : \n",
    "# RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU. \n",
    "''' J'ai donc modifié la ligne 57 du fichier extract_img_feats.py en rajoutant 'map_location=torch.device('cpu')' dans la fonction torch.load '''\n",
    "# Encore une fois, affichage de 35 indices, environ 15-30min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (main, Dec 23 2022, 09:28:24) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
